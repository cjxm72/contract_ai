import os
from loguru import logger
import torch  # æå‰å¯¼å…¥torchï¼Œé¿å…åŠ è½½æ—¶å¯èƒ½çš„å¯¼å…¥é¡ºåºé—®é¢˜

from singleton import singleton


@singleton
class QwenModel:  # ç±»åæ›´æ–°ä¸ºQwenModelï¼Œæ˜ç¡®æ¨¡å‹ç±»å‹
    """
    ä½¿ç”¨å•ä¾‹æ¨¡å¼ç®¡ç†Qwenæ¨¡å‹ï¼ˆQwen2.5-7Bï¼‰
    é‡‡ç”¨8bité‡åŒ–ï¼ˆå…¼é¡¾è´¨é‡å’Œæ˜¾å­˜ï¼Œ16GBæ˜¾å­˜é€‚é…ï¼‰
    """

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self._is_loaded = False  # ç§»é™¤processorç›¸å…³å±æ€§

    def load_model(self):
        """åŠ è½½Qwenæ¨¡å‹ï¼ˆ8bité‡åŒ– + é€‚é…æœ€æ–°transformers APIï¼‰"""
        if self._is_loaded:
            logger.info("ğŸ“¦ æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return True

        try:
            # å¯¼å…¥å¿…è¦çš„ç»„ä»¶ï¼ŒQwenæ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨AutoModelForCausalLM
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

            # æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆç¡®ä¿æŒ‡å‘Qwenæ¨¡å‹çš„å®é™…ç›®å½•ï¼‰
            MODEL_ROOT = "/eos_pool/build/modelscope/models/Qwen3-14B"

            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(MODEL_ROOT):
                logger.error(f"âŒ Qwenæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_ROOT}")
                return False

            logger.info(f"â³ æ­£åœ¨ä» {MODEL_ROOT} åŠ è½½æ¨¡å‹{MODEL_ROOT.split('/')[-1]}...")

            # 1. åŠ è½½tokenizerï¼ˆQwenéœ€è¦trust_remote_code=Trueï¼‰
            self.tokenizer = AutoTokenizer.from_pretrained(
                MODEL_ROOT,
                trust_remote_code=True,
                padding_side="left"  # Qwenæ¨èleft paddingï¼Œé¿å…ç”Ÿæˆæˆªæ–­
            )
            # è¡¥å……pad_tokenï¼ˆQwenéƒ¨åˆ†ç‰ˆæœ¬é»˜è®¤æœªå®šä¹‰ï¼Œç”¨eos_tokenæ›¿ä»£ï¼‰
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # 2. é…ç½®8bité‡åŒ–å‚æ•°ï¼ˆæ›¿æ¢åŸ4bité…ç½®ï¼Œé€‚é…BitsAndBytesï¼‰
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,  # æ ¸å¿ƒï¼šå¯ç”¨8bité‡åŒ–ï¼ˆæ›¿æ¢load_in_4bitï¼‰
                bnb_8bit_use_double_quant=True,  # åŒé‡åŒ–ä¼˜åŒ–ï¼Œå…¼é¡¾æ˜¾å­˜å’Œè´¨é‡
                bnb_8bit_quant_type="nf8",  # 8bitæ¨èé‡åŒ–ç±»å‹ï¼ˆnf8é€‚é…è¯­ä¹‰ä»»åŠ¡ï¼‰
                bnb_8bit_compute_dtype=torch.float16,  # è®¡ç®—ç²¾åº¦ï¼Œé¿å…è´¨é‡æŸå¤±
                bnb_8bit_quant_storage=torch.float16,  # é‡åŒ–å­˜å‚¨ç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
                bnb_8bit_use_llm_int8_skip_modules=["lm_head"]  # è·³è¿‡è¾“å‡ºå±‚é‡åŒ–ï¼Œæå‡ç”Ÿæˆè´¨é‡
            )

            bnb_config_4 = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_storage = torch.float16,  # é‡åŒ–å­˜å‚¨ç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
                bnb_4bit_use_llm_int8_skip_modules = ["lm_head"]  # è·³è¿‡è¾“å‡ºå±‚é‡åŒ–ï¼Œæå‡ç”Ÿæˆè´¨é‡
            )

            # 3. åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨AutoModelForCausalLMï¼Œé€‚é…è¯­è¨€æ¨¡å‹ï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_ROOT,
                dtype=torch.float16,  # åŸºç¡€è®¡ç®—ç²¾åº¦
                trust_remote_code=True,  # Qwenéœ€è¦åŠ è½½è‡ªå®šä¹‰ä»£ç 
                device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼‰
                quantization_config=bnb_config_4,  # ä¼ å…¥bité‡åŒ–é…ç½®8/
                low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜å ç”¨
                offload_buffers=False,  # æ–°å¢ï¼šå¸è½½éæ ¸å¿ƒç¼“å†²åŒºåˆ°CPUï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜
                offload_state_dict=False
            )

            self._is_loaded = True
            logger.info(f"âœ… {MODEL_ROOT.split('/')[-1]}æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"âŒæ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            # åŠ è½½å¤±è´¥æ—¶é‡ç½®çŠ¶æ€
            self.model = None
            self.tokenizer = None
            self._is_loaded = False
            return False

    def release_resources(self):
        """é‡Šæ”¾æ¨¡å‹èµ„æº"""
        if not self._is_loaded:
            logger.info("ğŸ“­ æ¨¡å‹æœªåŠ è½½ï¼Œæ— éœ€é‡Šæ”¾")
            return

        try:
            import gc

            logger.info("ğŸ”„ å¼€å§‹é‡Šæ”¾æ¨¡å‹èµ„æº...")

            # æ¸…é™¤æ¨¡å‹å’Œtokenizer
            if self.model is not None:
                del self.model
                self.model = None
            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None

            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            self._is_loaded = False
            logger.info("âœ… æ¨¡å‹èµ„æºé‡Šæ”¾å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ èµ„æºé‡Šæ”¾å¤±è´¥: {e}")

    def is_loaded(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self._is_loaded

    def get_model(self):
        """è·å–æ¨¡å‹ç»„ä»¶ï¼ˆä»…è¿”å›modelå’Œtokenizerï¼‰"""
        if not self._is_loaded:
            logger.warning("âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
            return None, None
        return self.model, self.tokenizer

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆè¡¥å……é‡åŒ–ç±»å‹ï¼‰"""
        return {
            'model_loaded': self.model is not None,
            'tokenizer_loaded': self.tokenizer is not None,
            'is_loaded': self._is_loaded,
            'quantization': '8bit'  # æ˜ç¡®æ ‡æ³¨é‡åŒ–ç±»å‹
        }