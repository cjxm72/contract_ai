import asyncio
import json
import re
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from typing_extensions import TypedDict as TypedDictExt

try:
    from langchain.memory import ConversationBufferMemory
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
    from langchain_community.chat_message_histories import ChatMessageHistory
    ConversationBufferMemory = None

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from loguru import logger

from util.PandocSettings import openai_settings
from model_manager import QwenModel


class ContractState(TypedDictExt):
    """åˆåŒå¤„ç†çŠ¶æ€"""
    input_data: Dict[str, Any]
    conversation_history: List[Dict[str, str]]
    user_notes: List[str]
    asked_questions: List[str]
    round_idx: int
    info_complete: bool
    contract_content: str
    template_key: Optional[str]
    max_rounds: int


class LangChainContractManager:
    """åŸºäºLangChainçš„åˆåŒç®¡ç†å™¨"""
    
    def __init__(self, template_key: Optional[str] = None):
        self.local_model = None
        self.model_mode = "online"
        self.temperature = 0.3
        self.max_tokens = openai_settings.max_tokens or 4048
        self.timeout = openai_settings.timeout_seconds if openai_settings.use_local_model else openai_settings.timeout
        self.max_rounds = 3
        self.selected_template_key = template_key
        # åˆå§‹åŒ–è®°å¿†ï¼ˆå¦‚æœConversationBufferMemoryå¯ç”¨ï¼‰
        if ConversationBufferMemory is not None:
            self.memory = ConversationBufferMemory(
                return_messages=True,
                memory_key="chat_history"
            )
        else:
            self.memory = None
        
        self._initialize_model_pipeline()
        self._load_contract_templates()
        self._build_workflows()
    
    def _initialize_model_pipeline(self):
        """æ ¹æ®é…ç½®åŠ è½½æœ¬åœ°æ¨¡å‹æˆ–å‡†å¤‡åœ¨çº¿é…ç½®"""
        self.temperature = openai_settings.temperature if hasattr(openai_settings, "temperature") else self.temperature
        self.max_tokens = openai_settings.max_tokens or self.max_tokens

        if openai_settings.use_local_model:
            self.local_model = QwenModel()
            if self.local_model.load_model():
                self.model_mode = "local"
                logger.info("âœ… ä½¿ç”¨æœ¬åœ°Qwenæ¨¡å‹è¿›è¡Œæ¨ç†")
                # åˆ›å»ºæœ¬åœ°æ¨¡å‹åŒ…è£…å™¨
                self.llm = self._create_local_llm()
                return
            logger.warning("âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢ä¸ºåœ¨çº¿API")
            self.model_mode = "online"

        # åˆ›å»ºåœ¨çº¿æ¨¡å‹
        self.llm = ChatOpenAI(
            model=openai_settings.model,
            api_key=openai_settings.api_key,
            base_url=openai_settings.base_url,
            temperature=self.temperature,
            max_tokens=min(self.max_tokens, 4048),
            timeout=self.timeout,
            max_retries=openai_settings.max_retries if hasattr(openai_settings, "max_retries") else 1,
        )
        self.model_mode = "online"
    
    def _create_local_llm(self):
        """åˆ›å»ºæœ¬åœ°æ¨¡å‹åŒ…è£…å™¨ï¼ˆå…¼å®¹LangChainæ¥å£ï¼‰"""
        from langchain_core.language_models import BaseChatModel
        from langchain_core.outputs import ChatGeneration, ChatResult
        from langchain_core.messages import AIMessage
        
        class LocalQwenLLM(BaseChatModel):
            """æœ¬åœ°Qwenæ¨¡å‹åŒ…è£…å™¨"""
            
            def __init__(self, model, tokenizer, temperature, max_tokens):
                super().__init__()
                self.model = model
                self.tokenizer = tokenizer
                self.temperature = temperature
                self.max_tokens = max_tokens
            
            @property
            def _llm_type(self) -> str:
                return "local_qwen"
            
            def _generate(
                self,
                messages,
                stop=None,
                run_manager=None,
                **kwargs
            ):
                # å°†æ¶ˆæ¯è½¬æ¢ä¸ºæç¤ºè¯
                prompt = self._messages_to_prompt(messages)
                
                inputs = self.tokenizer(prompt, return_tensors="pt")
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                
                output_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=min(self.max_tokens, 1024),
                    temperature=self.temperature,
                    do_sample=True,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id
                )
                decoded = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
                if decoded.startswith(prompt):
                    decoded = decoded[len(prompt):]
                
                message = AIMessage(content=decoded.strip())
                generation = ChatGeneration(message=message)
                return ChatResult(generations=[generation])
            
            def _messages_to_prompt(self, messages):
                """å°†LangChainæ¶ˆæ¯è½¬æ¢ä¸ºæç¤ºè¯"""
                prompt = ""
                for msg in messages:
                    if isinstance(msg, SystemMessage):
                        prompt += f"System: {msg.content}\n"
                    elif isinstance(msg, HumanMessage):
                        prompt += f"User: {msg.content}\n"
                    elif isinstance(msg, AIMessage):
                        prompt += f"Assistant: {msg.content}\n"
                prompt += "Assistant: "
                return prompt
        
        model, tokenizer = self.local_model.get_model()
        return LocalQwenLLM(model, tokenizer, self.temperature, self.max_tokens)

    def _load_contract_templates(self):
        """åŠ è½½åˆåŒæ¨¡æ¿"""
        current_dir = Path(__file__).parent
        self.template_dir = current_dir.parent / "templates"
        self.templates = {}

        logger.info(f"ğŸ“ æŸ¥æ‰¾æ¨¡æ¿ç›®å½•: {self.template_dir}")

        if not self.template_dir.exists():
            error_msg = f"âŒ æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {self.template_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)

        # ä¿®æ”¹ï¼šåŒæ—¶æ”¯æŒ .md å’Œ .docx æ–‡ä»¶
        md_files = list(self.template_dir.glob("*.md"))
        docx_files = list(self.template_dir.glob("*.docx"))
        template_files = md_files + docx_files

        if not template_files:
            error_msg = f"âŒ æ¨¡æ¿ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡æ¿æ–‡ä»¶(.mdæˆ–.docx): {self.template_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)

        logger.info(f"ğŸ” æ‰¾åˆ° {len(template_files)} ä¸ªæ¨¡æ¿æ–‡ä»¶")

        for template_file in template_files:
            try:
                if template_file.suffix == '.md':
                    with open(template_file, 'r', encoding='utf-8') as f:
                        self.templates[template_file.stem] = f.read()
                elif template_file.suffix == '.docx':
                    # éœ€è¦å®‰è£… python-docx åº“: pip install python-docx
                    from docx import Document
                    doc = Document(template_file)
                    content = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                    self.templates[template_file.stem] = content
                logger.info(f"âœ… åŠ è½½æ¨¡æ¿: {template_file.name}")
            except Exception as e:
                error_msg = f"âŒ åŠ è½½æ¨¡æ¿å¤±è´¥ {template_file.name}: {e}"
                logger.error(error_msg)
                raise Exception(error_msg)

    def _select_template(self, contract_type: str = "", template_key: Optional[str] = None) -> str:
        """é€‰æ‹©åˆåŒæ¨¡æ¿ï¼Œæ”¯æŒå¤–éƒ¨å‚æ•°æŒ‡å®š"""
        # ä¼˜å…ˆä½¿ç”¨æ„é€ å‡½æ•°ä¼ å…¥çš„æ¨¡æ¿
        if self.selected_template_key and self.selected_template_key in self.templates:
            logger.info(f"âœ… ä½¿ç”¨ä¸»å‡½æ•°æŒ‡å®šçš„æ¨¡æ¿: {self.selected_template_key}")
            return self.selected_template_key

        # å¦‚æœæä¾›äº†å¤–éƒ¨å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨
        if template_key and template_key in self.templates:
            logger.info(f"âœ… ä½¿ç”¨æŒ‡å®šçš„æ¨¡æ¿: {template_key}")
            return template_key

        # å¦‚æœæ²¡æœ‰æä¾›æˆ–æä¾›çš„æ¨¡æ¿ä¸å­˜åœ¨ï¼Œåˆ™è‡ªä¸»é€‰æ‹©
        if contract_type:
            contract_type_lower = contract_type.lower()
            for key in self.templates.keys():
                if key.lower() in contract_type_lower or contract_type_lower in key.lower():
                    logger.info(f"âœ… è‡ªåŠ¨é€‰æ‹©æ¨¡æ¿: {key}")
                    return key

        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªæ¨¡æ¿ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›default
        if self.templates:
            default_key = list(self.templates.keys())[0]
            logger.info(f"âœ… ä½¿ç”¨é»˜è®¤æ¨¡æ¿: {default_key}")
            return default_key

        logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ¨¡æ¿")
        return 'default'
    
    def _should_stop_questioning(self, reviewer_message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢æé—®"""
        if not reviewer_message:
            return False
        status = self._extract_status_tag(reviewer_message)
        if status == "å®Œæ•´":
            return True
        if status == "ä¸å®Œæ•´":
            return False

        normalized = reviewer_message.replace(" ", "")
        if "ä¸å®Œæ•´" in normalized or "æ— æ³•ç”Ÿæˆ" in normalized or "éœ€è¡¥å……" in normalized:
            return False
        if "ä¿¡æ¯å®Œæ•´" in normalized and "ä¸å®Œæ•´" not in normalized:
            return True
        if "å¯ä»¥ç”ŸæˆåˆåŒ" in normalized and "ä¸å¯ä»¥" not in normalized:
            return True
        return False

    def _extract_status_tag(self, reviewer_message: str) -> str:
        """è§£ææ¨¡å‹è¾“å‡ºä¸­çš„çŠ¶æ€æ ‡ç­¾"""
        if not reviewer_message:
            return ""
        match = re.search(r"ã€çŠ¶æ€ã€‘\s*(å®Œæ•´|ä¸å®Œæ•´)", reviewer_message)
        if match:
            return match.group(1)
        return ""

    def _extract_questions(self, ai_response: str) -> List[str]:
        """ä»AIå›ç­”ä¸­æå–æå‡ºçš„é—®é¢˜"""
        questions = []
        if not ai_response:
            return questions

        question_match = re.search(r"ã€é—®é¢˜ã€‘\s*([^\n]+(?:\n[^\n]+)*)", ai_response)
        if question_match:
            question_text = question_match.group(1).strip()
            for line in question_text.split('\n'):
                line = line.strip()
                if line and line != "æ— " and len(line) > 5:
                    questions.append(line)
        else:
            question_sentences = re.findall(r'[^ã€‚ï¼ï¼Ÿ]*[ï¼Ÿ?][^ã€‚ï¼ï¼Ÿ]*', ai_response)
            for q in question_sentences:
                q = q.strip()
                if q and len(q) > 5:
                    questions.append(q)

        return questions
    
    def _check_contract_length(self, contract_content: str, max_length: int = 50000) -> bool:
        """æ£€æŸ¥åˆåŒé•¿åº¦ï¼Œè¶…è¿‡é™åˆ¶è¿”å›True"""
        return len(contract_content) > max_length
    
    def _split_contract_generation(self, prompt: str, max_chunk_size: int = 3000) -> List[str]:
        """åˆ†æ®µç”ŸæˆåˆåŒï¼Œè¿”å›å„æ®µå†…å®¹"""
        # å°†æç¤ºè¯åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«ç”Ÿæˆ
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ ¹æ®åˆåŒç»“æ„åˆ†æ®µ
        chunks = []
        # å¯ä»¥æŒ‰æ®µè½æˆ–ç« èŠ‚åˆ†å‰²æç¤ºè¯
        # æš‚æ—¶è¿”å›å•ä¸ªæç¤ºè¯ï¼Œç”±ç”Ÿæˆå‡½æ•°å¤„ç†åˆ†æ®µ
        return [prompt]
    
    def _merge_contract_segments(self, segments: List[str]) -> str:
        """åˆå¹¶åˆåŒåˆ†æ®µ"""
        # æ·»åŠ AIç”Ÿæˆå£°æ˜
        header = """---
ã€é‡è¦æç¤ºã€‘æœ¬åˆåŒç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¸ºå‚è€ƒèŒƒæœ¬ï¼Œå¿…é¡»ç»è¿‡ä¸“ä¸šæ³•å¾‹äººå‘˜å®¡æ ¸å’Œä¿®æ”¹åæ–¹å¯ä½¿ç”¨ã€‚ä¸å¾—ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½äº§ç”Ÿæ³•å¾‹é£é™©ã€‚
---

"""
        return header + "\n\n".join(segments)
    
    async def _generate_contract_segment(self, segment_prompt: str) -> str:
        """ç”ŸæˆåˆåŒçš„ä¸€ä¸ªåˆ†æ®µ"""
        try:
            response = await self.llm.ainvoke([HumanMessage(content=segment_prompt)])
            return response.content.strip()
        except Exception as e:
            logger.error(f"âŒ åˆåŒåˆ†æ®µç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _build_workflows(self):
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        # å®¡æŸ¥äº¤äº’å·¥ä½œæµ
        self.review_workflow = self._build_review_workflow()
        # åˆåŒç”Ÿæˆå·¥ä½œæµ
        self.generation_workflow = self._build_generation_workflow()
    
    def _build_review_workflow(self):
        """æ„å»ºå®¡æŸ¥äº¤äº’å·¥ä½œæµ"""
        workflow: StateGraph[ContractState] = StateGraph(ContractState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("initial_analysis", self._initial_analysis_node)
        workflow.add_node("user_input_round0", self._user_input_round0_node)
        workflow.add_node("continue_analysis", self._continue_analysis_node)
        workflow.add_node("check_completeness", self._check_completeness_node)
        workflow.add_node("user_input_roundN", self._user_input_roundN_node)
        
        # è®¾ç½®å…¥å£
        workflow.set_entry_point("initial_analysis")
        
        # æ·»åŠ è¾¹
        workflow.add_edge("initial_analysis", "user_input_round0")
        workflow.add_edge("user_input_round0", "continue_analysis")
        workflow.add_conditional_edges(
            "continue_analysis",
            self._should_continue_review,
            {
                "continue": "check_completeness",
                "complete": END
            }
        )
        workflow.add_conditional_edges(
            "check_completeness",
            self._should_ask_user,
            {
                "ask": "user_input_roundN",
                "complete": END
            }
        )
        workflow.add_edge("user_input_roundN", "continue_analysis")
        
        return workflow.compile()
    
    def _build_generation_workflow(self):
        """æ„å»ºåˆåŒç”Ÿæˆå·¥ä½œæµ"""
        workflow: StateGraph[ContractState] = StateGraph(ContractState)
        
        workflow.add_node("prepare_generation", self._prepare_generation_node)
        workflow.add_node("generate_contract", self._generate_contract_node)
        workflow.add_node("check_length", self._check_length_node)
        workflow.add_node("split_generate", self._split_generate_node)
        workflow.add_node("merge_contract", self._merge_contract_node)
        
        workflow.set_entry_point("prepare_generation")
        
        workflow.add_edge("prepare_generation", "generate_contract")
        workflow.add_conditional_edges(
            "check_length",
            self._should_split,
            {
                "split": "split_generate",
                "merge": "merge_contract"
            }
        )
        workflow.add_edge("generate_contract", "check_length")
        workflow.add_edge("split_generate", "merge_contract")
        workflow.add_edge("merge_contract", END)
        
        return workflow.compile()
    
    # å®¡æŸ¥å·¥ä½œæµèŠ‚ç‚¹
    async def _initial_analysis_node(self, state: ContractState):
        """åˆå§‹åˆ†æèŠ‚ç‚¹"""
        prompt = self._prepare_initial_context(state["input_data"], "", state["asked_questions"], is_first_round=True)
        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        ai_response = response.content.strip()[:300]
        
        new_questions = self._extract_questions(ai_response)
        state["asked_questions"].extend(new_questions)
        state["conversation_history"].append({
            "role": "assistant",
            "content": ai_response,
            "name": "SemanticReviewer"
        })
        state["info_complete"] = False  # ç¬¬0è½®å¼ºåˆ¶ä¸å®Œæ•´
        
        print(f"\n{'=' * 60}")
        print("ğŸ¤– AIåˆ†æç»“æœï¼š")
        print(ai_response)
        print(f"{'=' * 60}")
        
        return state
    
    async def _user_input_round0_node(self, state: ContractState):
        """ç¬¬0è½®ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹"""
        print("\nğŸ’¬ è¯·æ ¹æ®ä»¥ä¸Šé—®é¢˜è¡¥å……ä¿¡æ¯ï¼ˆå¤šè¡Œè¾“å…¥ï¼Œç©ºè¡Œç»“æŸï¼Œè¾“å…¥exitå¯ç»ˆæ­¢ï¼‰ï¼š")
        user_input = self._get_user_input().strip()
        
        if not user_input:
            logger.warning("âš ï¸ ç”¨æˆ·æœªæä¾›è¡¥å……ä¿¡æ¯")
        elif user_input.lower() in {"exit", "quit", "q", "åœæ­¢", "é€€å‡º"}:
            raise ValueError("ç”¨æˆ·ç»ˆæ­¢å¯¹è¯")
        else:
            state["user_notes"].append(user_input)
            state["conversation_history"].append({
                "role": "user",
                "content": user_input,
                "name": "User"
            })
            state["round_idx"] += 1
        
        return state
    
    async def _user_input_roundN_node(self, state: ContractState):
        """ç¬¬Nè½®ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹"""
        print("\nğŸ’¬ è¯·æ ¹æ®ä»¥ä¸Šé—®é¢˜è¡¥å……ä¿¡æ¯ï¼ˆå¤šè¡Œè¾“å…¥ï¼Œç©ºè¡Œç»“æŸï¼Œè¾“å…¥exitå¯ç»ˆæ­¢ï¼‰ï¼š")
        user_input = self._get_user_input().strip()
        
        if not user_input:
            logger.warning("âš ï¸ ç”¨æˆ·æœªæä¾›è¡¥å……ä¿¡æ¯")
        elif user_input.lower() in {"exit", "quit", "q", "åœæ­¢", "é€€å‡º"}:
            raise ValueError("ç”¨æˆ·ç»ˆæ­¢å¯¹è¯")
        else:
            state["user_notes"].append(user_input)
            state["conversation_history"].append({
                "role": "user",
                "content": user_input,
                "name": "User"
            })
            state["round_idx"] += 1
        
        return state
    
    async def _continue_analysis_node(self, state: ContractState):
        """ç»§ç»­åˆ†æèŠ‚ç‚¹"""
        if state["round_idx"] >= state["max_rounds"]:
            state["info_complete"] = False  # è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œæ ‡è®°ä¸ºä¸å®Œæ•´ä½†ç»§ç»­ç”Ÿæˆ
            return state
        
        print(f"\nğŸ”„ ç¬¬ {state['round_idx']} è½®ï¼šåŸºäºç”¨æˆ·å›ç­”ç»§ç»­åˆ†æ")
        merged_supplements = "\n".join(f"- {note}" for note in state["user_notes"])
        prompt = self._prepare_initial_context(
            state["input_data"], 
            merged_supplements, 
            state["asked_questions"], 
            is_first_round=False
        )
        
        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        ai_response = response.content.strip()[:300]
        state["info_complete"] = self._should_stop_questioning(ai_response)
        
        new_questions = self._extract_questions(ai_response)
        state["asked_questions"].extend(new_questions)
        state["conversation_history"].append({
            "role": "assistant",
            "content": ai_response,
            "name": "SemanticReviewer"
        })
        
        return state
    
    async def _check_completeness_node(self, state: ContractState):
        """æ£€æŸ¥å®Œæ•´æ€§èŠ‚ç‚¹"""
        # åªæœ‰ä¸å®Œæ•´æ—¶æ‰è¾“å‡ºåˆ°ç»ˆç«¯
        if not state["info_complete"]:
            print(f"\n{'=' * 60}")
            print("ğŸ¤– AIåˆ†æç»“æœï¼š")
            print(state["conversation_history"][-1]["content"])
            print(f"{'=' * 60}")
            
            if state["round_idx"] >= state["max_rounds"]:
                print("\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ¬¡ï¼Œå°†åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆåˆåŒèŒƒæœ¬")
        else:
            print(f"\n{'=' * 60}")
            print("âœ… ä¿¡æ¯å®Œæ•´ï¼Œå¼€å§‹ç”ŸæˆåˆåŒ")
            print("[DEBUG] ğŸ¤– AIåˆ†æç»“æœï¼š")
            print(state["conversation_history"][-1]["content"])
            print(f"{'=' * 60}")
        
        return state
    
    def _should_continue_review(self, state: ContractState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­å®¡æŸ¥"""
        if state["info_complete"] or state["round_idx"] >= state["max_rounds"]:
            return "complete"
        return "continue"
    
    def _should_ask_user(self, state: ContractState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è¯¢é—®ç”¨æˆ·"""
        # å¦‚æœä¸å®Œæ•´ä¸”æœªè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œéœ€è¦è¯¢é—®ç”¨æˆ·
        if not state["info_complete"] and state["round_idx"] < state["max_rounds"]:
            return "ask"
        return "complete"
    
    # ç”Ÿæˆå·¥ä½œæµèŠ‚ç‚¹
    async def _prepare_generation_node(self, state: ContractState):
        """å‡†å¤‡ç”ŸæˆèŠ‚ç‚¹"""
        final_data = state["input_data"].copy()
        if state["user_notes"]:
            final_data["user_supplements"] = "\n\n".join(state["user_notes"])
        
        # æ”¯æŒå¤–éƒ¨æŒ‡å®šæ¨¡æ¿
        template_key = state.get("template_key")
        contract_type = state["input_data"].get('contract_type', '').lower()
        selected_template = self._select_template(contract_type, template_key)
        state["template_key"] = selected_template
        
        return state
    
    async def _generate_contract_node(self, state: ContractState):
        """ç”ŸæˆåˆåŒèŠ‚ç‚¹"""
        generation_prompt = self._build_generation_prompt(state["input_data"], state["template_key"])
        response = await self.llm.ainvoke([HumanMessage(content=generation_prompt)])
        state["contract_content"] = response.content.strip()
        
        return state
    
    async def _check_length_node(self, state: ContractState):
        """æ£€æŸ¥é•¿åº¦èŠ‚ç‚¹"""
        # é•¿åº¦æ£€æŸ¥é€»è¾‘åœ¨æ¡ä»¶è¾¹ä¸­å¤„ç†
        return state
    
    def _should_split(self, state: ContractState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ®µ"""
        if self._check_contract_length(state["contract_content"]):
            return "split"
        return "merge"
    
    async def _split_generate_node(self, state: ContractState):
        """åˆ†æ®µç”ŸæˆèŠ‚ç‚¹"""
        generation_prompt = self._build_generation_prompt(state["input_data"], state["template_key"])
        segments = self._split_contract_generation(generation_prompt)
        
        contract_segments = []
        for segment_prompt in segments:
            segment_content = await self._generate_contract_segment(segment_prompt)
            contract_segments.append(segment_content)
        
        state["contract_content"] = self._merge_contract_segments(contract_segments)
        return state
    
    async def _merge_contract_node(self, state: ContractState):
        """åˆå¹¶åˆåŒèŠ‚ç‚¹"""
        if not self._check_contract_length(state["contract_content"]):
            # å¦‚æœä¸éœ€è¦åˆ†æ®µï¼Œæ·»åŠ å£°æ˜å³å¯
            if not state["contract_content"].startswith("---"):
                header = """---
ã€é‡è¦æç¤ºã€‘æœ¬åˆåŒç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¸ºå‚è€ƒèŒƒæœ¬ï¼Œå¿…é¡»ç»è¿‡ä¸“ä¸šæ³•å¾‹äººå‘˜å®¡æ ¸å’Œä¿®æ”¹åæ–¹å¯ä½¿ç”¨ã€‚ä¸å¾—ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½äº§ç”Ÿæ³•å¾‹é£é™©ã€‚
---

"""
                state["contract_content"] = header + state["contract_content"]
        
        return state
    
    def _prepare_initial_context(self, input_data: Dict[str, Any], supplements: str = "", asked_questions: List[str] = None, is_first_round: bool = False) -> str:
        """å‡†å¤‡è¯­ä¹‰åˆ†æä¸Šä¸‹æ–‡"""
        if asked_questions is None:
            asked_questions = []

        context = """è¯·åˆ†æä»¥ä¸‹åˆåŒä¿¡æ¯çš„å®Œæ•´æ€§ï¼Œè¯†åˆ«ç¼ºå¤±æˆ–ä¸æ˜ç¡®çš„ä¿¡æ¯ã€‚æ³¨æ„ï¼šç”Ÿæˆçš„åˆåŒä»…ä¸ºèŒƒæœ¬ï¼Œéœ€è¦äººå·¥è¿›ä¸€æ­¥ä¿®æ”¹å®Œå–„ï¼Œå› æ­¤æœ‰ç‚¹ä¸¥æ ¼ä½†ä¸å¿…è¿‡äºä¸¥æ ¼ã€‚

    ã€åˆåŒåŸºæœ¬ä¿¡æ¯ã€‘
    """

        for key, value in input_data.items():
            if value:
                context += f"- {key}: {value}\n"

        if supplements:
            context += f"""
ã€ç”¨æˆ·æœ€æ–°è¡¥å……ã€‘
{supplements}
"""

        if asked_questions:
            context += f"""
ã€å·²æå‡ºçš„é—®é¢˜ï¼ˆè¯·å‹¿é‡å¤æé—®ï¼‰ã€‘
"""
            for i, q in enumerate(asked_questions, 1):
                context += f"{i}. {q}\n"

        if is_first_round:
            context += """
    ã€ç¬¬ä¸€è½®åˆ†æè¦æ±‚ï¼ˆé‡è¦ï¼‰ã€‘
    è¿™æ˜¯ç¬¬ä¸€è½®åˆ†æï¼Œå¿…é¡»æ‰¾å‡ºè‡³å°‘1-3ä¸ªé—®é¢˜å‘ç”¨æˆ·æé—®ï¼Œä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œæ•´ä½“å­—æ•°ä¸å¾—è¶…è¿‡300å­—ã€‚
    é‡ç‚¹å…³æ³¨ï¼š
    1. JSONä¸­æœªæåŠçš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚ï¼šå…·ä½“äº¤ä»˜æ ‡å‡†ã€éªŒæ”¶æ ‡å‡†ã€ä¿å¯†æ¡æ¬¾ç­‰ï¼‰
    2. ä¿¡æ¯ä¸æ˜ç¡®çš„åœ°æ–¹ï¼ˆå¦‚ï¼šé‡‘é¢å•ä½ã€æ—¶é—´èŠ‚ç‚¹ã€è´£ä»»åˆ’åˆ†ç­‰ï¼‰
    3. å³ä½¿ä¿¡æ¯çœ‹èµ·æ¥å®Œæ•´ï¼Œä¹Ÿè¦æ‰¾å‡ºå¯ä»¥è¿›ä¸€æ­¥æ˜ç¡®æˆ–è¡¥å……çš„ç‚¹
    4. è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼ˆç¦æ­¢æ·»åŠ å¤šä½™æ®µè½ï¼‰ï¼š
       ã€çŠ¶æ€ã€‘å®Œæ•´/ä¸å®Œæ•´ï¼ˆä»…å¯äºŒé€‰ä¸€ï¼‰å¯ä»¥ç”ŸæˆåˆåŒå¿…é¡»æ˜¾ç¤ºå®Œæ•´
       ã€ç»“è®ºã€‘ä¸€å¥è¯æ¦‚æ‹¬ç»“æœï¼Œè‹¥å®Œæ•´éœ€åŒ…å«"å¯ä»¥ç”ŸæˆåˆåŒ"
       ã€é—®é¢˜ã€‘è‹¥çŠ¶æ€ä¸ºä¸å®Œæ•´ï¼Œåˆ—å‡ºæ•°æ¡å¾…è¡¥å……é—®é¢˜ï¼›è‹¥çŠ¶æ€ä¸ºå®Œæ•´ï¼Œå¡«å†™"æ— "
    é™¤éä¿¡æ¯çœŸçš„éå¸¸å®Œæ•´ä¸”æ— ä»»ä½•å¯ä¼˜åŒ–ç©ºé—´ï¼Œå¦åˆ™å¿…é¡»æé—®ã€‚
    """

        context += """
    è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚åˆ†æå¹¶è¾“å‡ºï¼Œæ•´ä½“å­—æ•°ä¸å¾—è¶…è¿‡300å­—ï¼š
    1. åˆ¤æ–­å…³é”®ä¿¡æ¯æ˜¯å¦åŸºæœ¬å®Œå–„ï¼Œæ³¨æ„è¿™æ˜¯åˆåŒèŒƒæœ¬ï¼Œä¸éœ€è¦100%å®Œæ•´
    2. è‹¥å­˜åœ¨ç¼ºå¤±æˆ–ä¸æ˜ç¡®ä¿¡æ¯ï¼Œä»…åˆ—å‡ºæœ€å…³é”®çš„æ•°æ¡é—®é¢˜ï¼ˆé¿å…é‡å¤å·²é—®è¿‡çš„é—®é¢˜ï¼‰
    3. è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼ˆç¦æ­¢æ·»åŠ å¤šä½™æ®µè½ï¼‰ï¼š
       ã€çŠ¶æ€ã€‘å®Œæ•´/ä¸å®Œæ•´ï¼ˆä»…å¯äºŒé€‰ä¸€ï¼‰å¯ä»¥ç”ŸæˆåˆåŒå¿…é¡»æ˜¾ç¤ºå®Œæ•´
       ã€ç»“è®ºã€‘ä¸€å¥è¯æ¦‚æ‹¬ç»“æœï¼Œè‹¥å®Œæ•´éœ€åŒ…å«"å¯ä»¥ç”ŸæˆåˆåŒ"
       ã€é—®é¢˜ã€‘è‹¥çŠ¶æ€ä¸ºä¸å®Œæ•´ï¼Œåˆ—å‡ºæ•°æ¡å¾…è¡¥å……é—®é¢˜ï¼›è‹¥çŠ¶æ€ä¸ºå®Œæ•´ï¼Œå¡«å†™"æ— "
    4. è¦æ±‚å…³é”®è¦ç´ ï¼ˆä¸»ä½“ã€æœåŠ¡å†…å®¹ã€é‡‘é¢/æ”¯ä»˜ã€æœŸé™ç­‰ï¼‰åŸºæœ¬æ˜ç¡®"
    5. ä¸å…è®¸è¾“å‡ºjsonçš„å­—æ®µï¼Œå³é”®å€¼å¯¹çš„é”®ï¼Œæ¶‰åŠä¹Ÿä¸å…è®¸ï¼ï¼ï¼ï¼
    6. ä¸¥ç¦é‡å¤æå‡ºå·²åˆ—åœ¨"å·²æå‡ºçš„é—®é¢˜"ä¸­çš„ç›¸åŒæˆ–ç±»ä¼¼é—®é¢˜"""

        if is_first_round:
            context += """
    7. ã€ç¬¬ä¸€è½®å¼ºåˆ¶è¦æ±‚ã€‘å¿…é¡»æ‰¾å‡ºè‡³å°‘1-3ä¸ªé—®é¢˜ï¼Œé™¤éä¿¡æ¯çœŸçš„éå¸¸å®Œæ•´ä¸”æ— ä»»ä½•å¯ä¼˜åŒ–ç©ºé—´"""

        context += """

    è¯·å¼€å§‹åˆ†æï¼š"""

        return context

    def _build_generation_prompt(self, input_data: Dict[str, Any], template_key: Optional[str] = None) -> str:
        """æ„å»ºåˆåŒç”Ÿæˆæç¤ºè¯"""
        if template_key is None:
            contract_type = input_data.get('contract_type', '').lower()
            template_key = self._select_template(contract_type)
        
        template_content = self.templates.get(template_key, "")

        prompt = f"""è¯·åŸºäºä»¥ä¸‹å®Œæ•´ä¿¡æ¯ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åˆåŒèŒƒæœ¬ï¼š

ã€åˆåŒä¿¡æ¯ã€‘
{json.dumps(input_data, ensure_ascii=False, indent=2)}"""

        if template_content:
            prompt += f"""

ã€å‚è€ƒæ¨¡æ¿æ ¼å¼ã€‘
{template_content}

**é‡è¦**ï¼šå‚è€ƒæ¨¡æ¿çš„ç»“æ„å’Œæ¡æ¬¾ç±»å‹ï¼Œå¹¶å¡«ç©ºä¿®æ”¹ï¼Œä½†ä¸è¦ç›´æ¥å®Œå…¨å¤åˆ¶æ¨¡æ¿å†…å®¹ã€‚åŸºäºæä¾›çš„å…·ä½“ä¿¡æ¯ç”Ÿæˆå…¨æ–°çš„åˆåŒå†…å®¹ã€‚"""

        prompt += """

**ç”Ÿæˆè¦æ±‚**ï¼š
1. åœ¨åˆåŒå¼€å¤´å¿…é¡»æ·»åŠ ä»¥ä¸‹å£°æ˜ï¼ˆä½¿ç”¨é†’ç›®æ ‡è®°ï¼‰ï¼š
   "---
   ã€é‡è¦æç¤ºã€‘æœ¬åˆåŒç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¸ºå‚è€ƒèŒƒæœ¬ï¼Œå¿…é¡»ç»è¿‡ä¸“ä¸šæ³•å¾‹äººå‘˜å®¡æ ¸å’Œä¿®æ”¹åæ–¹å¯ä½¿ç”¨ã€‚ä¸å¾—ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½äº§ç”Ÿæ³•å¾‹é£é™©ã€‚
   ---"
2. åŸºäºæä¾›çš„å…·ä½“ä¿¡æ¯ç”ŸæˆåˆåŒå†…å®¹
3. æ¡æ¬¾å®Œæ•´ã€åˆæ³•ã€ä¸“ä¸š
4. ä½¿ç”¨è§„èŒƒçš„åˆåŒè¯­è¨€
5. åŒ…å«å¿…è¦çš„åˆåŒè¦ç´ 
6. è¾“å‡ºçº¯åˆåŒå†…å®¹ï¼ˆåŒ…å«å¼€å¤´çš„å£°æ˜ï¼‰ï¼Œä¸è¦åŒ…å«é¢å¤–è¯´æ˜

è¯·ç”ŸæˆåˆåŒï¼š"""

        return prompt
    
    def _get_user_input(self) -> str:
        """è·å–ç”¨æˆ·å¤šè¡Œè¾“å…¥ï¼ˆç©ºè¡Œç»“æŸï¼‰"""
        lines = []
        print("è¯·è¾“å…¥å†…å®¹ï¼ˆç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            try:
                line = input()
                if not line.strip():
                    break
                lines.append(line)
            except EOFError:
                break
        return "\n".join(lines)
    
    async def process_contract_interactive(self, input_data: Dict[str, Any], template_key: Optional[str] = None) -> Dict[str, Any]:
        """äº¤äº’å¼å¤„ç†åˆåŒè¯·æ±‚ï¼ˆä½¿ç”¨LangGraphå·¥ä½œæµï¼‰"""
        initial_state: ContractState = {
            "input_data": input_data,
            "conversation_history": [],
            "user_notes": [],
            "asked_questions": [],
            "round_idx": 0,
            "info_complete": False,
            "contract_content": "",
            "template_key": template_key,
            "max_rounds": self.max_rounds
        }
        
        try:
            # æ‰§è¡Œå®¡æŸ¥å·¥ä½œæµ
            print(f"\n{'=' * 60}")
            print("ğŸ¤– AIæ­£åœ¨æ‰§è¡Œè¯­ä¹‰çº§å®Œæ•´æ€§åˆ†æ...")
            print(f"{'=' * 60}")
            print(f"\nğŸ”„ ç¬¬ 0 è½®ï¼šåˆå§‹åˆ†æ")
            
            review_state = await self.review_workflow.ainvoke(initial_state)
            
            # æ‰§è¡Œç”Ÿæˆå·¥ä½œæµ
            if review_state.get("info_complete"):
                print("\nğŸ¯ ä¿¡æ¯å®Œæ•´ï¼Œå¼€å§‹ç”ŸæˆåˆåŒèŒƒæœ¬...")
            else:
                print("\nğŸ¯ åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆåˆåŒèŒƒæœ¬ï¼ˆè¯·æ³¨æ„ï¼šæ­¤èŒƒæœ¬éœ€è¦è¿›ä¸€æ­¥ä¿®æ”¹å®Œå–„ï¼‰...")
            
            final_state = await self.generation_workflow.ainvoke(review_state)
            
            return {
                "status": "success",
                "contract": final_state["contract_content"],
                "conversation": final_state["conversation_history"],
                "message": "åˆåŒç”ŸæˆæˆåŠŸ"
            }
            
        except ValueError as e:
            if "ç”¨æˆ·ç»ˆæ­¢å¯¹è¯" in str(e):
                return {
                    "status": "error",
                    "message": "ç”¨æˆ·ç»ˆæ­¢å¯¹è¯",
                    "conversation": initial_state.get("conversation_history", [])
                }
            raise
        except Exception as e:
            logger.error(f"âŒ åˆåŒå¤„ç†å¤±è´¥: {e}")
            raise

    def run(self, template_key: Optional[str] = None):
        """è¿è¡Œæ¼”ç¤ºä¸»å‡½æ•°ï¼ˆåŒæ­¥å…¥å£ï¼Œå†…éƒ¨è°ƒç”¨å¼‚æ­¥é€»è¾‘ï¼‰"""
        asyncio.run(self._async_run(template_key))

    async def _async_run(self, template_key: Optional[str] = None):
        """å¼‚æ­¥æ ¸å¿ƒä¸»å‡½æ•°ï¼ˆäº¤äº’å¼å¤„ç†æ¨¡å¼ï¼‰"""
        try:
            logger.info("ğŸš€ å¯åŠ¨åˆåŒç”Ÿæˆæ¼”ç¤ºç¨‹åºï¼ˆLangChainç‰ˆï¼‰")
            print("=" * 70)
            print("ğŸ¤– æ™ºèƒ½åˆåŒç”Ÿæˆç³»ç»Ÿï¼ˆLangChain + LangGraphï¼‰")
            print("=" * 70)

            current_dir = Path(__file__).parent
            input_dir = current_dir.parent / "input"

            logger.info(f"ğŸ” æŸ¥æ‰¾inputç›®å½•: {input_dir}")
            print(f"ğŸ” æŸ¥æ‰¾inputç›®å½•: {input_dir}")

            if not input_dir.exists():
                logger.error(f"âŒ inputç›®å½•ä¸å­˜åœ¨: {input_dir}")
                print(f"âŒ é”™è¯¯: inputç›®å½•ä¸å­˜åœ¨")
                return

            json_files = list(input_dir.glob("*.json"))
            if not json_files:
                logger.warning(f"âš ï¸ inputç›®å½•ä¸­æ²¡æœ‰JSONæ–‡ä»¶: {input_dir}")
                print(f"âŒ é”™è¯¯: inputç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
                return

            print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªåˆåŒæ–‡ä»¶:")
            for i, json_file in enumerate(json_files, 1):
                print(f"  {i}. {json_file.name}")

            # æ–°å¢ï¼šæ˜¾ç¤ºæ¨¡æ¿ä¿¡æ¯
            if template_key:
                if template_key in self.templates:
                    print(f"âœ… ä½¿ç”¨æŒ‡å®šæ¨¡æ¿: {template_key}")
                else:
                    print(f"âš ï¸ æŒ‡å®šæ¨¡æ¿ '{template_key}' ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©")
                    template_key = None
            else:
                print("âœ… å°†ä½¿ç”¨è‡ªåŠ¨æ¨¡æ¿é€‰æ‹©")

            print(f"\n{'=' * 70}")
            print("ğŸ” äº¤äº’å¼å¤„ç†æ¨¡å¼")
            print(f"{'=' * 70}")

            for i, json_file in enumerate(json_files, 1):
                print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(json_files)}: {json_file.name}")

                with open(json_file, 'r', encoding='utf-8') as f:
                    input_data = json.load(f)

                print(f"ğŸ“‹ åˆå§‹ä¿¡æ¯:")
                for key, value in input_data.items():
                    print(f"  - {key}: {value}")

                # ä¿®æ”¹ï¼šä¼ é€’æŒ‡å®šçš„æ¨¡æ¿
                result = await self.process_contract_interactive(input_data, template_key)

                if result['status'] == 'success':
                    output_dir = current_dir.parent / "output"
                    output_dir.mkdir(exist_ok=True)
                    output_file = output_dir / f"{json_file.stem}.md"

                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(result['contract'])

                    print(f"âœ… åˆåŒå·²ä¿å­˜: {output_file}")
                    print(f"ğŸ“„ åˆåŒé¢„è§ˆ:\n{result['contract'][:300]}...")
                else:
                    print(f"âŒ ç”Ÿæˆå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")

                if i < len(json_files):
                    continue_choice = input(f"\nç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶? (y/n): ").strip().lower()
                    if continue_choice != 'y':
                        break

            print(f"\n{'=' * 70}")
            print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ")
            print(f"{'=' * 70}")

        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        finally:
            self.release_resources()
    
    def release_resources(self):
        """é‡Šæ”¾æ¨¡å‹èµ„æº"""
        if self.local_model and self.local_model.is_loaded():
            self.local_model.release_resources()
            logger.info("âœ… å·²é‡Šæ”¾æœ¬åœ°æ¨¡å‹èµ„æº")
        if self.memory is not None:
            self.memory.clear()
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.release_resources()


if __name__ == "__main__":
    # é…ç½®è¯¦ç»†æ—¥å¿—
    logger.remove()
    logger.add(
        "contract_system.log",
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {file}:{line} | {message}",
        rotation="10 MB",
        retention="7 days"
    )
    logger.add(
        sys.stdout,
        level="DEBUG",
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>"
    )

    # è¿è¡Œæ¼”ç¤ºç¨‹åº
    template_name = "ç‰©è”ç½‘æµé‡åˆåŒ"  # åœ¨è¿™é‡Œè®¾ç½®æƒ³è¦çš„æ¨¡æ¿åç§°
    manager = LangChainContractManager()
    manager.run(template_key=template_name)
